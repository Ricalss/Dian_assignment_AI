材料学院——冯雄飞的文档
# 基本知识
## 数据集:

分为训练集与测试集，分类问题中一般有相应的标签集。
一般的分类问题中：训练集用于输入到预测模型，得到输出，评价输出（损失函数），以及反向优化模型中的参数，使得预测效果更好。

## 模型：

人为设计的一种复杂函数，以达到相关的数据输入到这个模型中，该模型可以得到一个映射的值（该值用来反映输入数据的某种特征）。比如MNIST集中，输入一张图片，模型输出一个值来表达这张图片是哪个数字。然而这种函数很难认为创造，设计者只能创建一个架构，如CNN模型，但是当中无数的参数无法人工确定。参数初始随机赋值，后续优化都交给训练集和优化器。当确定函数的构架，优化器优化其中的参数，得到适合的参数，这就是个优秀的预测模型。

## 优化器：

优化器是利用模型中数据，以及输出的评价，反向优化模型中的参数。一般而言采用反向传播，计算各参数的梯度，再取所有可变参数的梯度值的负值乘上学习率（梯度的负值才是使损失函数值减小的方向），更改模型中的参数。反向传播、计算梯度是优化器中最重要的部分。

## 损失函数：

损失函数是用来评价模型预测结果的好坏的函数，常用的有交叉熵函数、softmax函数等等。交叉熵函数整合了softmax、log、NLLloss函数，其值越小说明模型预测效果越好。反向传播时计算梯度就是从损失函数的输出值loss开始的。

## 关于CNN模型：
   CNN层数越多，预测能力越强，预测问题越复杂
   笔者将卷积层与池化层看做一层，有些人将池化层单独做一层。

笔者设计的CNN模型：

## 正向传播：

layer1：卷积层--(bs, 1, 28, 28)-->规范化输出--(bs, 16, 28, 28)-->池化层--(bs, 16, 14, 14 )-->layer2

layer2:卷积层--(bs, 16, 14, 14)-->--(bs, 32, 14, 14)-->池化层--(bs, 32, 7, 7 )-->reshape

reshape--(bs, 32*7*7)-->layer3

layer3:线性层--(bs, 10)--规范化输出-->交叉熵函数

CrossEntropyLoss()--->Loss

## 反向传播：上述过程逆向求出各计算过程的当地梯度，再利用链式法则计算出所有参数的梯度，此即为反向传播

# 训练细节
BATCH_SIZE = 1000   学习率learning_rate = 0.001   训练次数num_epochs = 1 #因为数据计算比较慢，没有选择太大训练次数。

cnn-self中采用了nn.ReLU6()处理线性层输出的数据，保证最大值为6，防止数据爆炸。layer1/2也采用nn.BatchNorm2d()函数进行规范化防止数据爆炸。（nan、inf）

采用计时器，测试cnn-self实现的速度：一个样本（100，1, 28, 28）数据输入到model中大约耗时5.2s完成训练，完成一完整次训练耗时53分钟

# function实现

## 线性层

简单的矩阵叉乘，forward以及backward利用矩阵乘法实现。

注意到weight矩阵是（out_features, in_features），linear初始化矩阵用empty（）实属不恰当，会造成数据爆炸，将其改成randn（）

## 卷积层

forward思路：将卷积核cat+reshape成二维矩阵self.kernel_mat ,（kn, lens）; 输入input在padding后转换为self.pad_mat(bs,lens);

两者点乘利用广播机制得到（bs， kn， lens）向量，再还原到output（bs，kn， dim2， dim3）+三维的bias。
 
backward思路：ones拓展成（bs，kn， lens）三维矩阵，分别与self.kernel_mat、self.pad_mat广播相点乘，在还原为input、kernel维度即可。bias使用sum函数解决。

## 交叉熵函数

forward思路：softmax---->-log()---->mean(取标签对应的值的平均)

backward思路：直接去除表达式 对应label值：(exp(x0)/sum(exp(x))-1)/bs; 非label值：exp(x0)/sum(exp(x))/bs